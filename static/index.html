<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI-Sight</title>
    <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>
    <style>
      * {
        -webkit-user-select: none; /* Safari */
        -ms-user-select: none; /* IE 10 and IE 11 */
        user-select: none; /* Standard syntax */
      }

    </style>
  </head>
  <body class="h-screen w-screen overflow-hidden relative">
    <video
      id="webcam"
      autoplay
      playsinline
      class="absolute top-0 left-0 w-screen h-screen object-cover -z-10"></video>

    <div class="max-w-md m-auto h-screen relative">
      <header
        class="absolute top-0 left-0 w-full px-6 py-4 flex justify-between items-center z-50">
        <!-- Left icon -->
        <button class="w-14 h-14">
        </button>

        <!-- Center logo -->
        <div
          class="bg-[#34464D] px-8 py-3 rounded-[20px] border-2 border-white">
          <h1 class="text-white text-3xl font-bold">AI-Sight</h1>
        </div>

        <!-- Right icon -->
        <button
          class="w-14 h-14 flex items-center justify-center rounded-full border-2 border-white text-white">
          <!-- Info icon -->
          <svg
            xmlns="http://www.w3.org/2000/svg"
            class="w-8 h-8"
            fill="none"
            viewBox="0 0 24 24"
            stroke="currentColor">
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              stroke-width="2"
              d="M13 16h-1v-4h-1m1-4h.01M12 12h.01M12 16h.01" />
          </svg>
        </button>
      </header>

      <div
        class="absolute inset-0 -top-[20%] flex items-center justify-center pointer-events-none">
        <div class="w-[300px] h-[300px] relative">
          <!-- Top-left -->
          <div
            class="absolute top-0 left-0 w-10 h-10 border-t-4 border-l-4 border-white rounded-tl-[40px]"></div>
          <!-- Top-right -->
          <div
            class="absolute top-0 right-0 w-10 h-10 border-t-4 border-r-4 border-white rounded-tr-[40px]"></div>
          <!-- Bottom-left -->
          <div
            class="absolute bottom-0 left-0 w-10 h-10 border-b-4 border-l-4 border-white rounded-bl-[40px]"></div>
          <!-- Bottom-right -->
          <div
            class="absolute bottom-0 right-0 w-10 h-10 border-b-4 border-r-4 border-white rounded-br-[40px]"></div>
        </div>
      </div>

      <canvas id="gesturePad" width="300" height="300" class="absolute inset-x-1/2 -translate-x-1/2 bottom-14 w-[80%] h-40 px-3 py-2 bg-white rounded-2xl shadow-black"></canvas>
        
    <script>
      // WebSocket and state management
      let socket = null;
      let isConnected = false;
      let mediaStream = null;
      let audioContext = null;
      let audioProcessor = null;
      let webcamInterval = null;
      let isRecording = false;
      let currentCameraMode = "environment"; // 'environment' or 'user'

      // Audio playback management
      let audioQueue = [];
      let isPlaying = false;
      let playbackAudioContext = null;
      let audioBufferSource = null;
      let audioAnalyser = null;
      let visualizationInterval = null;
      let currentAudioEndTime = 0;
      let audioChunks = [];

      // Gesture Pad Variables
      let isDrawing = false;
      let gesturePoints = [];
      let lastX = 0;
      let lastY = 0;
      let touchStartTime = 0;
      let longPressTimer = null;

      // Sound management
      let soundEnabled = true;
      const sounds = {
        success: new Audio('/static/sounds/success.mp3'),
        error: new Audio('/static/sounds/error.mp3'),
        notification: new Audio('/static/sounds/notification.mp3'),
        connect: new Audio('/static/sounds/connect.mp3'), 
        disconnect: new Audio('/static/sounds/disconnect.mp3'),
        gesture: new Audio('/static/sounds/gesture.mp3'),
        startListening: new Audio('/static/sounds/startListening.mp3'),
        stopListening: new Audio('/static/sounds/stopListening.mp3'),
        alert: new Audio('/static/sounds/alert.mp3')
      };

      // DOM elements
      const toggleSoundBtn = document.getElementById("toggleSound");
      const responseText = document.getElementById("responseText");
      const webcamVideo = document.getElementById("webcam");

      async function requestMediaPermissions() {
        try {
          await navigator.mediaDevices.getUserMedia({
            video: true,
            audio: true,
          });
          console.log("Permissions granted for camera and microphone");
        } catch (error) {
          console.error("Permissions denied:", error);
          alert(
            "Please allow access to your camera and microphone to use this app."
          );
        }
      }

      // Initialize camera preview
      async function initCameraPreview() {
        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: false,
            video: {
              facingMode: currentCameraMode,
            },
          });
          webcamVideo.srcObject = mediaStream;
        } catch (error) {
          console.error("Error initializing camera preview:", error);
        }
      }

      // Toggle between front and back camera
      async function toggleCamera() {
        currentCameraMode = currentCameraMode === "environment" ? "user" : "environment";
        await initCameraPreview();
        playSound("notification");
      }

      // Capture single image and send to server
      async function captureSingleImage() {
        if (!isConnected || !socket || socket.readyState !== WebSocket.OPEN) {
            playSound("error");
            return;
        }

        try {
            const captureAndSend = () => {
              const canvas = document.createElement("canvas");
              canvas.width = webcamVideo.videoWidth;
              canvas.height = webcamVideo.videoHeight;
              const ctx = canvas.getContext("2d");
              ctx.drawImage(webcamVideo, 0, 0, canvas.width, canvas.height);

              canvas.toBlob(
                (blob) => {
                  const reader = new FileReader();
                  reader.onload = () => {
                    const base64data = reader.result.split(",")[1];
                    socket.send(
                      JSON.stringify({
                        type: "image",
                        data: base64data,
                      })
                    );
                  };
                  reader.readAsDataURL(blob);
                },
                "image/jpeg",
                0.7
              );
            };

            // Send first image immediately
            captureAndSend();
            
            // Send two more images at 5ms intervals
            setTimeout(captureAndSend, 2);
            setTimeout(captureAndSend, 8);
            setTimeout(() => {
              socket.send(
                  JSON.stringify({
                      type: "text",
                      data: "explain the image in detail",
                  })
              );
            }, 1000);

            playSound("success");
        } catch (error) {
            console.error("Error capturing images:", error);
            playSound("error");
        }
    }

      // Connect to WebSocket
      async function connectWebSocket() {
        if (isConnected) return;

        try {
          const protocol =
            window.location.protocol === "https:" ? "wss:" : "ws:";
          socket = new WebSocket(`${protocol}//${window.location.host}/ws`);

          socket.onopen = () => {
            isConnected = true;
            playSound("connect");
          };

          socket.onclose = () => {
            isConnected = false;
            setTimeout(connectWebSocket, 3000);
          };

          socket.onerror = (error) => {
            console.error("WebSocket error:", error);
          };

          socket.onmessage = async (event) => {
            // Handle binary audio data
            if (event.data instanceof Blob) {
              try {
                const arrayBuffer = await event.data.arrayBuffer();
                handleAudioData(arrayBuffer);
              } catch (error) {
                console.error("Blob processing error:", error);
              }
            } else if (event.data instanceof ArrayBuffer) {
              handleAudioData(event.data);
            }
            // Handle text messages
            else if (typeof event.data === "string") {
              try {
                const message = JSON.parse(event.data);
                if (message.type === "text") {
                  responseText.innerHTML += `<p>${message.data}</p>`;
                  responseText.scrollTop = responseText.scrollHeight;
                  playSound("notification");
                  checkForImportantMessage(message.data);
                }
              } catch (e) {
                console.error("Message parsing error:", e);
              }
            }
          };
        } catch (error) {
          console.error("Error connecting to WebSocket:", error);
          setTimeout(connectWebSocket, 3000);
        }
      }

      function handleAudioData(arrayBuffer) {
        // Convert to the correct audio format
        const audioData = convertAudioDataIfNeeded(arrayBuffer);

        // Queue the audio data
        audioQueue.push(audioData);

        // Start playback if we have enough buffered chunks
        const MIN_BUFFERED_CHUNKS = 3;
        if (audioQueue.length >= MIN_BUFFERED_CHUNKS && !isPlaying) {
          processAudioQueue();
        }
      }

      function convertAudioDataIfNeeded(buffer) {
        // Check if the buffer has a WAV header
        const header = new Uint8Array(buffer, 0, 4);
        const isWav =
          header[0] === 0x52 &&
          header[1] === 0x49 &&
          header[2] === 0x46 &&
          header[3] === 0x46;

        if (isWav) {
          return buffer;
        }

        // Convert raw PCM to WAV format
        return encodeRawPCMAsWAV(buffer, 1, 24000);
      }

      function encodeRawPCMAsWAV(buffer, numChannels, sampleRate) {
        const bytesPerSample = 2;
        const blockAlign = numChannels * bytesPerSample;
        const byteRate = sampleRate * blockAlign;
        const dataSize = buffer.byteLength;
        const fileSize = 36 + dataSize;

        const wavHeader = new ArrayBuffer(44);
        const view = new DataView(wavHeader);

        // Write WAV header
        writeString(view, 0, "RIFF");
        view.setUint32(4, fileSize, true);
        writeString(view, 8, "WAVE");
        writeString(view, 12, "fmt ");
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, numChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, byteRate, true);
        view.setUint16(32, blockAlign, true);
        view.setUint16(34, 16, true);
        writeString(view, 36, "data");
        view.setUint32(40, dataSize, true);

        // Combine header and PCM data
        const wavBuffer = new Uint8Array(
          wavHeader.byteLength + buffer.byteLength
        );
        wavBuffer.set(new Uint8Array(wavHeader), 0);
        wavBuffer.set(new Uint8Array(buffer), wavHeader.byteLength);

        return wavBuffer.buffer;
      }

      function writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }

      async function processAudioQueue() {
        if (audioQueue.length === 0) {
          isPlaying = false;
          currentAudioEndTime = 0;
          return;
        }

        // Don't start new playback if previous audio hasn't finished
        const now = playbackAudioContext ? playbackAudioContext.currentTime : 0;
        if (now < currentAudioEndTime) {
          return;
        }

        isPlaying = true;

        try {
          // Initialize audio context if needed
          if (!playbackAudioContext) {
            playbackAudioContext = new (window.AudioContext ||
              window.webkitAudioContext)({
              sampleRate: 24000,
            });

            audioAnalyser = playbackAudioContext.createAnalyser();
            audioAnalyser.fftSize = 64;
          }

          // Get the next chunk of audio
          const audioData = audioQueue.shift();

          // Decode the audio data
          const audioBuffer = await playbackAudioContext.decodeAudioData(
            audioData
          );
          const duration = audioBuffer.duration;

          // Create and configure audio source
          if (audioBufferSource) {
            audioBufferSource.stop();
            audioBufferSource.disconnect();
          }

          audioBufferSource = playbackAudioContext.createBufferSource();
          audioBufferSource.buffer = audioBuffer;

          // Connect to analyzer and destination
          audioBufferSource.connect(audioAnalyser);
          audioAnalyser.connect(playbackAudioContext.destination);

          // Start visualization
          // startAudioVisualization();

          // Schedule playback with precise timing
          const startTime = Math.max(
            playbackAudioContext.currentTime,
            currentAudioEndTime
          );
          audioBufferSource.start(startTime);
          currentAudioEndTime = startTime + duration;

          // When this chunk finishes, play the next one
          audioBufferSource.onended = () => {
            setTimeout(() => {
              processAudioQueue();
            }, 30); // Small delay between chunks
          };
        } catch (error) {
          console.error("Audio playback error:", error);
          setTimeout(() => {
            processAudioQueue();
          }, 100);
        }
      }

      // Start media recording
      async function startMediaRecording() {
        if (isRecording) return;

        try {
          isRecording = true;
          gesturePad.classList.add("recording");
          playSound("startListening")

          // Add audio to media stream
          const audioStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              sampleRate: 16000,
            },
            video: false,
          });

          // Set up audio processing
          audioContext = new (window.AudioContext || window.webkitAudioContext)(
            {
              sampleRate: 16000,
            }
          );

          const source = audioContext.createMediaStreamSource(audioStream);
          audioProcessor = audioContext.createScriptProcessor(4096, 1, 1);

          // Process audio chunks
          audioProcessor.onaudioprocess = (event) => {
            if (
              !isRecording ||
              !isConnected ||
              !socket ||
              socket.readyState !== WebSocket.OPEN
            )
              return;

            const inputData = event.inputBuffer.getChannelData(0);
            const pcmData = new Int16Array(inputData.length);
            for (let i = 0; i < inputData.length; i++) {
              pcmData[i] = Math.max(
                -32768,
                Math.min(32767, inputData[i] * 32767)
              );
            }

            try {
              socket.send(
                JSON.stringify({
                  type: "audio",
                  data: Array.from(pcmData),
                  sampleRate: audioContext.sampleRate,
                })
              );
            } catch (error) {
              console.error("Error sending audio:", error);
            }
          };

          source.connect(audioProcessor);
          audioProcessor.connect(audioContext.destination);

          // Send video frames periodically
          webcamInterval = setInterval(() => {
            if (!isRecording || !isConnected) return;

            try {
              const canvas = document.createElement("canvas");
              canvas.width = webcamVideo.videoWidth;
              canvas.height = webcamVideo.videoHeight;
              const ctx = canvas.getContext("2d");
              ctx.drawImage(webcamVideo, 0, 0, canvas.width, canvas.height);

              canvas.toBlob(
                (blob) => {
                  const reader = new FileReader();
                  reader.onload = () => {
                    const base64data = reader.result.split(",")[1];
                    socket.send(
                      JSON.stringify({
                        type: "image",
                        data: base64data,
                      })
                    );
                  };
                  reader.readAsDataURL(blob);
                },
                "image/jpeg",
                0.7
              );
            } catch (error) {
              console.error("Error capturing webcam frame:", error);
            }
          }, 500);
        } catch (error) {
          console.error("Error starting media recording:", error);
          stopMediaRecording();
        }
      }

      // Stop media recording
      function stopMediaRecording() {
        if (!isRecording) return;

        isRecording = false;
        gesturePad.classList.remove("recording");
        playSound("stopListening");

        if (audioProcessor) {
          audioProcessor.disconnect();
          audioProcessor = null;
        }

        if (mediaStream) {
          mediaStream.getAudioTracks().forEach((track) => track.stop());
        }

        if (webcamInterval) {
          clearInterval(webcamInterval);
          webcamInterval = null;
        }

        if (
          mediaStream &&
          !mediaStream
            .getVideoTracks()
            .some((track) => track.readyState === "live")
        ) {
          initCameraPreview();
        }
      }

      // Clean up audio playback
      function cleanupAudioPlayback() {
        if (audioBufferSource) {
          audioBufferSource.stop();
          audioBufferSource.disconnect();
          audioBufferSource = null;
        }
        if (playbackAudioContext) {
          playbackAudioContext
            .close()
            .catch((e) => console.log("AudioContext close error:", e));
          playbackAudioContext = null;
        }
        audioQueue = [];
        isPlaying = false;
        currentAudioEndTime = 0;
      }

      // Play a sound with error handling
      function playSound(soundName) {
        if (!soundEnabled) return;
        
        try {
          const sound = sounds[soundName];
          if (sound) {
            // Reset the audio to the beginning if it's already playing
            sound.pause();
            sound.currentTime = 0;
            
            // Play the sound
            sound.play().catch(err => {
              console.warn(`Sound play failed: ${err.message}`);
            });
          }
        } catch (error) {
          console.error("Error playing sound:", error);
        }
      }

      // Add this function with your other sound-related functions
      function checkForImportantMessage(text) {
        // List of keywords that might indicate important information
        const alertKeywords = [
          "caution", "warning", "danger", "careful", "attention", 
          "stop", "emergency", "obstacle", "hazard", "alert"
        ];
        
        // Check if the message contains any alert keywords
        const lowercaseText = text.toLowerCase();
        const isImportant = alertKeywords.some(keyword => 
          lowercaseText.includes(keyword)
        );
        
        if (isImportant) {
          playSound("alert"); // Play a more noticeable alert sound
        }
      }

      // Handle page visibility changes
      document.addEventListener("visibilitychange", () => {
        if (document.hidden) {
          stopMediaRecording();
          cleanupAudioPlayback();
        }
      });

      // Gesture Pad functionality
      const gesturePad = document.getElementById("gesturePad");
      const clearGestureBtn = document.getElementById("clearGesture");
      const ctx = gesturePad.getContext("2d");

      // Set up gesture pad
      function setupGesturePad() {
        // Clear the canvas
        ctx.fillStyle = "#f0f0f0";
        ctx.fillRect(0, 0, gesturePad.width, gesturePad.height);
        
        // Set up drawing properties
        ctx.strokeStyle = "#4285f4";
        ctx.lineWidth = 4;
        ctx.lineCap = "round";
        ctx.lineJoin = "round";
      }

      // Start drawing on the pad
      function startDrawing(e) {
        isDrawing = true;
        gesturePad.classList.add("drawing");
        
        // Get the starting position
        const rect = gesturePad.getBoundingClientRect();
        const clientX = e.clientX || (e.touches && e.touches[0].clientX);
        const clientY = e.clientY || (e.touches && e.touches[0].clientY);
        
        lastX = clientX - rect.left;
        lastY = clientY - rect.top;
        
        // Start a new gesture path
        gesturePoints = [{x: lastX, y: lastY}];
        
        // Start drawing
        ctx.beginPath();
        ctx.moveTo(lastX, lastY);
        
        // Set up long press detection
        touchStartTime = Date.now();
        longPressTimer = setTimeout(() => {
          if (isDrawing) {
            // Long press detected - start recording
            startMediaRecording();
          }
        }, 800); // 800ms for long press
      }

      // Draw on the pad
      function draw(e) {
        if (!isDrawing) return;
        
        // Prevent scrolling on touch devices
        e.preventDefault();
        
        // Get the current position
        const rect = gesturePad.getBoundingClientRect();
        const clientX = e.clientX || (e.touches && e.touches[0].clientX);
        const clientY = e.clientY || (e.touches && e.touches[0].clientY);
        
        const currentX = clientX - rect.left;
        const currentY = clientY - rect.top;
        
        // Draw the line
        ctx.lineTo(currentX, currentY);
        ctx.stroke();
        
        // Save the current point
        gesturePoints.push({x: currentX, y: currentY});
        
        // Update for next draw
        lastX = currentX;
        lastY = currentY;
        
        // Cancel long press if user moves too much
        if (gesturePoints.length > 5 && longPressTimer) {
          clearTimeout(longPressTimer);
          longPressTimer = null;
        }
      }

      // Stop drawing and process the gesture
      function stopDrawing() {
        if (!isDrawing) return;
        
        isDrawing = false;
        gesturePad.classList.remove("drawing");
        
        // Clear long press timer if it exists
        if (longPressTimer) {
          clearTimeout(longPressTimer);
          longPressTimer = null;
        }
        
        // Check if this was a long press (recording)
        if (isRecording) {
          stopMediaRecording();
          return;
        }
        
        // Check if this was a tap (short duration and minimal movement)
        const duration = Date.now() - touchStartTime;
        const isTap = duration < 300 && gesturePoints.length < 5;
        
        if (isTap) {
          // Single tap - capture image
          captureSingleImage();
          clearGesturePad();
          return;
        }
        
        // Process swipe gestures
        if (gesturePoints.length > 5) {
          processGesture(gesturePoints);
        } else {
          clearGesturePad();
        }
      }

      // Process the drawn gesture
      function processGesture(points) {
        // Get basic info about the gesture
        const startPoint = points[0];
        const endPoint = points[points.length - 1];
        const dx = endPoint.x - startPoint.x;
        const dy = endPoint.y - startPoint.y;
        const distance = Math.sqrt(dx * dx + dy * dy);
        
        // Determine gesture type (simplified for demo)
        let gestureType = "";
        
        // Simple horizontal swipe
        if (Math.abs(dx) > Math.abs(dy) && Math.abs(dx) > 50) {
          gestureType = dx > 0 ? "right swipe" : "left swipe";
        } 
        // Simple vertical swipe
        else if (Math.abs(dy) > Math.abs(dx) && Math.abs(dy) > 50) {
          gestureType = dy > 0 ? "down swipe" : "up swipe";
        }
        // Default - unknown pattern
        else {
          gestureType = "unknown pattern";
        }
        
        // Update status
        playSound("gesture");
        
        // Handle gesture actions
        if (gestureType === "left swipe" || gestureType === "right swipe") {
          toggleCamera();
        }
        
        // Clear the pad after processing
        clearGesturePad();
      }

      // Clear the gesture pad
      function clearGesturePad() {
        setupGesturePad();
        gesturePoints = [];
      }

      // Add event listeners for gestures
      gesturePad.addEventListener("mousedown", startDrawing);
      gesturePad.addEventListener("mousemove", draw);
      gesturePad.addEventListener("mouseup", stopDrawing);
      gesturePad.addEventListener("mouseleave", stopDrawing);

      // Touch support
      gesturePad.addEventListener("touchstart", startDrawing);
      gesturePad.addEventListener("touchmove", draw);
      gesturePad.addEventListener("touchend", stopDrawing);
      
      window.addEventListener("DOMContentLoaded", async () => {
        await requestMediaPermissions();
        await initCameraPreview(); // optionally start the preview immediately
        connectWebSocket(); // or any other setup
        setupGesturePad();
      });
    </script>
  </body>
</html>